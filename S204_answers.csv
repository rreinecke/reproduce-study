S204_01
Peace and Love.
nothing
""
Your mama
""
blABLABA
"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."
Beauty and the Beer
""
""
""
"Share not only model, but when using application: (1) Share the input and settings you have used to run the model. Share (if possible) the dataset that you have used to compare the simulation results with (when comparing simulations with real data). "
"Most data are already provided within the most publications. So why not using a software tool during the submission to check reproducibility of results based on the code and the data, "
Foster putting codes publicly available (e;g. through github)
"Publish data alongside research papers. With the documentation contained within the paper, the results should be reproducible."
"We need people who can help to improve reproducibility, as many researchers with a focus on analytics and conceptualization are not familiar with coding and licenses. Still those researchers have great ideas that should not be ranked lower just because those people are not familiar with this new, necessary movement"
"Publication of code and (exemplary, applied) data with the manuscript as a standard ? but also acknowledging the extra efforts required for this.
Teaching reproducibility (with respect to software, data and measurements) in environmental sciences.
Universities should provide service entities which guide, support and review scientific software development to avoid the autodidact diversity of styles and mistakes."
"provide better material to learn about reproducibility and it's benefits, standardize the teaching of coding skills and their ""best practices"" in MSc or at the latest PhD programms, allocate TIME (and thus money) for making work reproducible"
"In the climate community this is not very easily addressed. Reproducing the output of global climate models would be absurdly time and resource consuming. Reproducing the paper figures based on the author's code and data used is doable, but is it complete reproducibility?"
A more detailed description in the paper and a share option for code
"share code and data with others, stronger collaborative efforts rather than competitive mindest, 
reduce the amount of code developing by individual working groups and rely more on available codes and community efforts for code development for similar questions and applications."
develop slowly an obligation to improve reproducibility
nothing
Journals should enforce FAIR not just get everyone away with it
"The emphasis of reproducibility and novelty is killing sciences in the developing world, especially in over-exploited complex GW systems. "
"Shadow or Pair computing could be made the norm.

Submitting for publications and working on open source codes could be made the norms by the editors."
Increase human resources dedicated to it
publish the code with easy to use licenses.
"At my institution, we are lately developing a strong focus on reproducibility. Especially for young scientists who are starting their projects, e.g., PhD students (in particular those that come from a non-programmer background) this topic seems to be overwhelming. They are struggling to get familiar with the models, with programming, with their research projects, and they are now also asked to strongly focus on reproducibility, ideally to a level where they think about Continuous Integration, Conainerizing of models and scripts, and extensive documentation. This is a lot to ask of a beginner. What we often hear from them is that they are already severely time-constrained and would like to dedicate more time to reproducibility, but don't see how they can do it within their allotted time for their project/thesis. Reproducibility should become more of a standard procedure in modeling-associated research, but to actually make it happen, it seems that it needs to be planned into the design of the research projects right from the start, to ensure that there is enough time available that can be dedicated to make research reproducible, and to teach (young) scientists who are not yet familiar with the concept and the means to implement it."
"Provide best practice guidelines, funding, and recognition for reproducible science. The current currency is publications, not well documented, tested, and reproducible research. "
"I think that time is a very important factor. Many scientists work in projects that up to now often do not allocate time to the reproducibility process. So the time to invest in reproducibility competes with time to produce project results. I think it could help to actually allocate time (and thus funding) to making science reproducible in project proposals.
Often clear guidelines and courses for beginners on how to ensure reproducibility from the very beginning are missing. For a beginner the different steps and tasks can seem overwhelming. Teaching from the very beginning and setting out the rules for a minimum standard could help improve the reproducibility."
"We need a lot more training. And we need to acknowledge that this is a considerable time investment for PhD students and post-docs who are the main scientists developing code. Current projects have very tight timelines that are often not feasible because of poor planning and poor time management - this is without investing time in reproducibility. Time devoted to reproducibility needs to be factored into project proposals and projects need to be guided with a Scrum framework that allows scientists to quickly see what is working and what is not, and to promote working in teams (which would facilitate code review). A lot of coders and professors I know work as 'lone wolves' - but making code open source and reproducible will require us to help each other, work in teams, and promote a spirit of openness. Right now, no one in my project would have the skills to review my code. I would have to ask other lab members not paid by my project to do the work which is asking for a lot of time, especially since this person would not be familiar with the scientific goals of my model and the questions it is supposed to answer. Reproducibility is incredibly important and I think most people want to improve, but we need to be given the environment where we can make such a time investment. "
I would recommend reproducing the result by a colleague not part of the concerned project/paper before submission. I believe this will increase the confidence and reproducibility of the research. 
Communicate the workflow
"? Dedicate a few months at the beginning of a PhD or Postdoc position for a crashcourse in professional programming.

? Mandatory crashcourses for senior researchers and professors in how to *evaluate* code quality. If journal reviewers and supervisors have no idea what good software looks like, it will never be rewarded.

? Invite experienced software engineers as consultants for model software development.

? Reduce publication pressure! Quality must come before quantity.

? Design and manage research projects as real teamwork (not just the loose ?collaboration? where everybody just does their own thing). Teamwork like in Scrum!"
Most important part in my area is that input data have to become open source!
"Code and data publications should be treated as valuable as/equally to papers (including funding opportunities, best practices, peer-review obligation, importance for applications for scholarships/positions)."
""
"The science community and funding institutions should put as high value on a good sceintific code - reviewed, documented and published - as on scientific papers. This actually also applies to research data that are along with software less valued than a paper. This requires a shift in a mindset which has started, but will take some time (years-decades) till we are there. You can do a PhD with three published papers, but not with a paper + a documented open-source code and a documented doi-Dataset! This is a point where to start change things. Of course, this may open door to easy-way PhDs consisting of a few subroutines, an Excel-Table and a low-profile paper, but this is what the supervisor and finally the PhD commission should look at."
-
develop and maintain open access databases
code repository connected to scientific journals where the modelling study is published; funding for staff / coders in addition to funding for scientists.
increase funding
no suggestion
"Common standards have to be checked regularly. Many manuscripts are published too fast...
My supervisor (habilitated years ago) often told me to do / code / calculate things I know to be absolutely wrong. I refused to do what he asked, because he did not want to make research reproducible, but to make his papers be published very soon. For him its much more relevant that his studies are published fast, not that their content makes sence and is reproducible. 
I will leave science as soon as possible - I cannot longer support this."
"- Publishers, Editors, and Reviewers should require all source code to be made available in open repositories (if necessary with a restricted access like Zenodo).

- Universities should recognise Research Software (and related papers in journals such as JOSS) as publications and part of PhD theses. 

- Universities should teach how to increase computational reproducibility.

"
"Each institutions/university should make reproducibility a priority, create data bases like OpenABM website to upload open source codes."
"Agree on common tests for code, models. "
appreciating formulas within papers more - necessary to increase reproducibility 
"Increase mentions of code sharing and research reproducibility in research publications, include code review in peer-review process."
""
- 
--
"Not sure
"
"Require all code open source.
Publish code as publication."
not accept papers anymore that obviously have something to hide or are completely unwilling to share.
 
-
"When reviewing a manuscript (mine or other), I ask myself, can I make the same model from the information in the code? Are inputs explained? what assumptions were made? Making these things very clear can help. "
Provide resources to help with the open science
Formalize the conceptual modelling steps prior to program coding 
-
"more critical peer review on methodology/code (when supplied as supplementary material, no review is required/possible)
easy liscensing of code/research software
easy access to finished products"
-
Missing question in 21: Include software and data reproducibility requirements into funding guidelines of major funding agencies.
/
"SHARE! Share your data, share your code! "
-
It's not only about codes but also about data which should be published along with the code. I think the past decade has seen some progress in making codes publicly available.
"Show your work in appendix, attachment, online, etc"
no ideas 
"- I would love to discuss how to standardize reporting model setups. I think this should probably rather come from journals.  
- We need to have an honest discussion about why people don't share their code. My impression is that the answers are similar to why anonymous reviews should be maintained.  
- Researchers are rarely required to produce good code. I mean when was the last time someone took your code apart and tested it? And producing good code takes practice, but also the need to want to write well-structured and well-tested code. I think the problem can only be improved if we accept that all of us are part of it and that all of us probably produce research output that is not reproducible. 
- I would love people to point out positive examples of reproducibility. 


"
"Guidelines for reproducible software generation, not just reproducible research in general, would be helpful, in addition to funding for improving established research software."
organize respective hackathons
"Include all codes during publication of work, should be mandatory by all journals."
""
Institutional know-how passing on and being updated.
Organize within the institution. Create side projects within the institution where researchers can actually work and publish their code together. Make these side projects mandatory.
"From what I understood, for a researcher to be successful, he/she must publish as much new data and answer as much new questions as possible what let no space to test for reproducibility of previous works."
"build on scientific code of others, and develop it differentially (in the sense of changesets)."
"Move beyond the static paper and encourage to have the code as supplement, or do things like jupyter notebooks in addition or instead of a paper."
"Encourage / demand data & software to be published along with research papers.
Strengthen the role of peer-review (e.g. stronger acknowledge Publons) and post-publication review (see Pubpeer)."
"A big help would be to have code available as frequently as possible and it being part of the review process. Datasets should be easily made available. When those cannot be shared, present synthetic series and results to be replicated by the community."
none
better document and provide online supportive course s
"Present your code to your lab mates, receive critiques"
Require reproducibility by funding agencies. 
"Emphasize documentation, read-me?s. Transfer of information after someone finishes a PhD is so lacking because people are normally scrambling to finish, and don?t think to put their models in a shared drive"
"I am overwhelmed with the endless variety of licenses, programing languages, sharing repositories, the constant migration towards newer approaches that make older ones obsolete. I don't know how to, but if this could be streamlined it would be most useful to me. The community would need to agree to some standard sharing norms that we all should follow."
only reproducible Research for publication
-
""
"A lot of today's scientific endeavor is performed through a process akin to software development yet there is a blatant gap between the standards to which scientific code and industry code are held to. For the sake of reproducibility and as a service to many graduates that will pursue a career in the private sector, industry standards should be taught and enforced. Publishing code and data is not enough if the authors are the only persons able to make sense of it. Versioning, packaging, long-form documentation, testing, continuous integration and deployment should become widely accepted standards in scientific software development as they have become in the private sector."
"Encourage standards / best practices for developing, documenting, and archiving code, including scripts for pre-/post-processing inputs/output data."
I have moved into programming in a way that tries to always start from the barebones of the data and maybe hiding sections later but always keeping it in a direct line of order so people can see the transformation of the source data directly before being applied into a model. Rather than transforming and cleaning the data up in one step and then presenting this data because this can cause issues with reproducibility as there will be a raw and cleaned data set but not always a clear distinction on which should be submitted.
"Code review within my research group has been an incredible process for improving the readability, structure, and best practices of my code. Code review events or conferences might be a good way to improve reproducibility."
"Give researchers the resources they need to learn to produce readable, well documented code and have consistent standards across journals regarding code and data availability."
Make a common aggreement 
"Open access code, git versioning, data repository (even included in git LFS), collaborated in a structured agile development workspace"
"Include more code in papers and describe it
Better/required documentation"
"Enforcing coding practices (documentation)
"
Share codes or softwares but overall share the input files.
Do not publish papers that relying on code and data do not provide them.
Stop using proprietary close-source software. Ever.
""
"The models that we use need to be open-source. Also, sharing the scripts (e.g. via GitHub) that we use for data post-processing would be useful and maybe even necessary."
Make reproducibility more understood by management and funding agencies
Journals should mandate reproducibility and include it in the review process.
"reward more the work that helps achieving reproducibility! for istance at the moment a paper presenting a new, clearer implementation of existing models/methods would have a hard time being published (reviewers would say ""it's not new!"") though it could be very impactful. Whereas a paper presenting a ""new"" model/method, which maybe is only a small incremental addition to an existing one, will be much easier to publish, and even more so if it is coded in a ""new"" package, which however only add to the complexity and fragmentation of the landscape! "
""
"We are moving towards the right direction. Code should be attached to publications along with ALL input data and the reviewers should be able just to run it to see if it works, if it generates the same outputs presented in the paper and eventually to check if all steps are correctly implemented. "
"Just share the code with good documentation and comments. So many high impact journals have no code documentation at all (even recent ones, despite apparent journal requirements). There is a huge burden of time for researchers to do this, though. "
More classes at university level on coding and/or workshops for more advanced researchers
"Include the code as annnex to the manuscript with a clear description of the steps used to get to the presented results (i.e. imputs, paramenters type and values, followed procedure, exceptions, approximations...)"
""
Open Repositories; Foster Open Source as good practice
Be honest
"Value more in CVs as peer revirewed publications code publications, code repositories, documentation writing ...
Provide more founding for this in applied science research poposals.
Need to have permanent staff in research institutions to support researcher for code reviewing documenting sharing."
not idea
Continous training is very important for aweraness creation. 
Run collective projects trying to reproduce selected studies
Publish matehenatical models.
no ideas
"A lot.  It is hard to summarize ideas succinctly in a comment like this, but see 
https://doi.org/10.1016/j.envsoft.2020.104888
http://doi.org/10.1016/j.envsoft.2019.03.020
http://dx.doi.org/10.1016/j.envsoft.2020.104753
https://doi.org/10.1061/(ASCE)WR.1943-5452.0001215"
"All the inputs used for the computation must be rigorously described, and also the methodology for make sure that the produced results are correct (e.g.: convergence studies in CFD)"
"Thanks for taking up the idea. Really important  issue. However for me reproducibility in modelling in hydrology would not mean just coding issues, the questionnaire is a bit biased toward formal criteria concerning coding. Reproducibility of research in mathematical modelling (including statistical models) in   hydrology is a  more complex issue.  I doubt, that even being offered the codes, all raw data and a detailed  description of the work done, it would be easy to reproduce someones results nowadays. These include subjective interpretations  of partial results and decisions taken based on these. Hydrology is not a lab science.  "
"One challenge that I see with reproducibility is the need for openly accessible data. Individuals who fund field campaigns that generate data feel somewhat ""used"" by modellers who do not obtain funding to generate data and in some cases expect that all data should be freely available. Essentially, if one researcher goes through the trouble of collecting and funding data why should the rewards of their efforts go to others before they have had the opportunity of fully exploiting that data. Additionally, if you put a temporal embargo on data availability so that data is private for some duration this may prevent students or PIs from publishing findings involving that data within that time period because of journal data sharing requirements. Perhaps using synthetic data is an option for demonstrating analysis and validity without exploiting data generators. "
I am not really sure why you think this is necessary
-
"The computational cost of replicating a global, continental, large-sample hydrological studies is too high. A sollution would be to create a demonstration dataset, which is a subset of the whole study, for testing reproducibility.  "
"I am not so sure that in our field reproducibility MUST be a condition for publication. It is not only a matter of considering the code. It works with data, which, depending on several circumstances, might have been acquired to a very high cost in financial and physical terms. Sharing this data might have certain non-convenient related aspects. For instance, multiple analyses might be associated to a unique data set. Writing a manuscript, depending on job-related conditions, may take a while. In the meanwhile, between a published work instant, some other research group might be working with ""your"" data and publish one of your ideas, before you, who actually collected the data; all of this, just because for publishing your past research you were obliged to share all of your data and code just because of ""reproducibility"". Further, it is my oppinion that ""doubting"" on our research results just because we do not share everything, including software and data, is not that  correct, nor that logical. Shall we doubt on what has been already published?... on the work of Beven, Refsgaard, Abbot, etc (hydrology)? Beven have shared TOPMODEL, most probably becasuse he was always interested in so, but al;so because its original development was financed explicitly with that purpose in mind... The Danish effort of Abbot and Refsgaard, MIKE SHE, is commercial... what will happen when I do research with a commercial software and cannot share it... even if I share the data, the research won't be 100% replicable.... does it mean that the research is bad and as such non-publicable? "
Impose to publish with open source model codes
Every paper should provide code and data for running and plotting every figure in it.
None
A reproducibility-test should be part of the review-process. 
Make sure it works as described
"Encourage code releases or links to source-code repositories as part of the manuscript acceptance/publication process, similar to how data is now commonly required to be released. There also needs to be an acknowledgement of the importance of releasing code in funding opportunities, and specific funds allocated for that. "
"Funding dedicated to this would be great. Stricter requirements (e.g., code review with publication) could be useful, but I worry about unintended consequences and the practicality - e.g., might become more difficult to find reviewers, more difficult for early career researchers in computational fields to publish relative to competitiors in non-computational fields. "
""
""
Move towards requiring all source code and data (that does not have legal restrictions on its use) to be published or deposited in a publicly available respository for all publications. 
Having an AGU or University wide standard to follow in our discipline would be nice as an early career scientist in order to follow best practices. 
"Hire more research software engineers that oversee sustainable software development practices and help with standardising, testing, and publishing research software."
"Journals should require code/data for publication where possible. Code review would be great, but it would probably be too time consuming for volunteer reviewers. Maybe journals could hire some data scientists to just check the code for correctness as a separate part of the review?"
"Journals should move towards requiring experimental reproducibility as a part of criteria for publication.
This means abandoning the publish or perish model as a basis for professional advancement.
To enable this, collaborations to assess reproducibility should be professionally rewarded."
"Develop community standards for transparency and open-source softwater development (especially in the research community)
Develop funder requirements that scientific code and data be publicly available.  i.e., treat software similarly to data and technology, with a degree of public accessibility for publicly-funded research, but the opportunity to protect and commercialize resulting IP"
maybe give more credit to work/effort of developing model/code.
Encourage publishing of code online in repostries like github.
more funding sources to improve reproducibility
"Mostly an attitude & values thing. 

- Value publication of workflows and data similar to scientific findings
- Encourage the citing of dataset/code DOIs (in addition to any papers describing said data/code)
- Move away from the ""I needed time to create this code and I don't want others to benefit from my work"" attitude (by doing e.g. the two things above)


"
-
"-Providing step-by-step supporting information besides the results. 
-Listing all the assumptions.

"
Include/require resources in project/grant proposals for making research reprodicable.  Offer more guidance and resources to groups from institutions.
""
"recognize that just sharing the code does not gaurantee reprodicibility. Results are very dependent on libraries used, often even operating systems used (famous example where MRI results were treated differently on mac vs windows machines in genetics research because the file sorting algorithm was different for those operating systems)

It is therefore 
- important to consider the entire compute environment used when working towards fully reproducible results
- try to keep this compute environment as lightweight, portable and machine agnostic as possible. Currently containers are a great way of achieving this.

Rolf Hut (yes, I know I broke annonimity...) "
"Journals should push for open and reproducible code and open data. If researchers do not want to share their code/data, they should be able to give a good reason.
While clear guidelines are helpful, it should also be made relatively easy. There's already a lot we have to do as researchers, so there's limited capacity to also become a ""professional software developer"". I think being too demanding (e.g. as a journal) might put people off, so we should balance the requirements with the effort they involve. That's also true for reviewing. Code reviewing is generally a great idea, but it will make reviewing more time consuming. 
Then I think we should make a distinction depending on the type of paper/code. If it's a paper that presents a new model, for example, then I would require a more thorough code review compared to a paper that just uses that model to produce results. Generally, we should distinguish between research software, i.e. where the focus is on the software itself, and research that uses software. Well written and documented code is always helpful, but quite often specific code used to setup a model for a specific case study simply isn't reused that often. So I think it is potentially unnecessary to expect the same standards (e.g. documentation) in that case."
"Funding programs offering more funding to develop software for science. Often there are no good funding options to follow up on code developed as part of science research projects. If funding for software for science were more broadly available could have better maintained, accessible, and usable software."
b
"My use of code pertains to processing data, modeling (using a mixture of existing modeling programs + my own analyses), and plotting. So, my response here does NOT pertain only to software development, but the full workflow of computational research. That said, what would be helpful to me in making my code more reproducible would be clear and simple guidelines and templates -- for the specific code environment (e.g., R, Python) that I use -- things that REDUCE time and effort on doing reproducible/open research, not things that increase time and effort for me and students.

The COMPLEXITY of reproducibility is often underestimated. This issue has been a major topic within the social sciences, and there now exist a nice collection of resources that necessarily reflect the broad range of what is considered ""reproducible"" and there are insights and resources portable to the geosciences, see e.g. https://www.bitss.org/"
"Quality of code on github is too uneven, some of it first-rate, some of it awful. MATLAB File Exchange is a more reliable venue to look for code about something I work on.
For reproducibility, I've used Code Ocean and plan to use it again. It establishes not just a place for the code, but also for an environment to run it. Therefore, reviewers can test the code without downloading and installing on their own system.
(I ran into a problem trying to review some Jupyter notebooks. I found them difficult to install on my system. I'm not a Python expert, so it would have been nicer to have it on Code Ocean.)"
""
There is must be a plateform globally where students from all over the world may interact each other about there similar research projects and this plateform must be supported from professional institutions like in form of scholarships/fellowships etc.
Have more allocated time (i.e. and that relates closely to funding) to prepare codes that are relatively easy to read by others in order to be able to share them. 
""
"Include these concepts in courses taught to undergrads and graduate students, and enforce transparency policies with journals."
"My departement does not have enouhg funding to get enough licenses for non open access software/programming languages such as MATLAB. I believe, however, that producing code or software with open access tools would improve reporducibility as more people would be in a position to actually use, review (and perhaps improve) new and existing tools."
--
Models needs data and produce data. data should go with the FAIR priciples
"We should increase collaboration across all climate/environment/water disciplines and the biggest challenge around me is to learn languages like R, Linux, C++ etc."
"Key is to assume very little to no experience when documenting your code. In general, good documentation is key, it is surprising how little it happens."
I don't know.
""
"There should be more courses on ""code documentation"" and ""publishing your code"" e.g. as public packages - not only at the university/institution level but internationally as well. From my experience, most people code for their own purposes, but they are willing (or even happy) to share their code, but they feel embarassed of its structure and functionality. To overcome this, they do not only need to learn the best practices, but also how to publish code; from small packages for simple tasks to complex packages for complex tasks. For the latter, often time to maintain these packages is lacking ? this should be accounted for somehow. "
"Pay people who conduct peer review, and require reproducability be taken into account in the peer review process"
"The most effective measures would be to change the incentives and provide learning opportunities. Currently, the incentive is to publish a lot and not worry about quality/reproducibility. As long as we are measured on how many papers are published in top journals, reproducibility will be at best a low priority. If funding/hiring/promotion is based on quality and reproducibility, then we'll invest in that. Teaching skills like https://software-carpentry.org fills in the gaps to get people to this standard."
"Conduct more seminars, webinars, conferences."
Improve sharing culture of open access data - data is often the problem
Open Science 
"Put reproducibility as ""mandatory"" in the publication of our scientific results"
not sure
"Make code publication mandatory, including working examples (at least small sample dataset). As witht he EGU copernicus journals, require data publication on some repository. If the latter is not possible due to data restrictions, require data repository of processed dataset (e.g. alternated values - same data disstribution and trend ...) and the entire post-processing workflow."
Make data and code publicly available and thoroughly documented
Require code to be published/available if it is used/referenced in a manuscript.
"Hybrid research with computational and data collection/creation and data product production goals... Besides writing papers.... They suffer from time pressure to fulfill ideal standards of documentation and have everything reviewed. Time wise this is not possible and it is a perverse thing to ask to young researchers unless also older PhD thesis requirements are dropped in number of chapters etc. I also think that open data should be 'reviewed by usage', after its publication, and be allowed to be corrected/updated/version managed without having to make a new doi or whatever.. 'living' data or 'living code' (services) is/are more important than published static data and code (archives) "
""
""
""
""
ask for providing codes during article submission
Publish program in open source code 
"In my country (UK) there is very little funding for good coding practice and almost no professional incentives for such activities (i.e., reproducibility and production of open source software has never come up in an appraisal or promotion case for me). 
Maintaining software for the community is a lot of work and my employer would much rather see me write papers with that time. 
Universities like citations: more standarisation of software citation might change this balance of incentives. 
There needs to be more funding for research software engineers."
a
Demand that the code is peer reviewed by qualified peer reviewers
""
Encourage open source code and compiled tools for reproducibility and wider application
Have our journals require access to all code just like access to all data.
"have continuos, trackable workflow, from data gathering to publications "
"Blind peer reviews seem to be the most essential step for any science publication.  I have experienced too many ""names"" getting their work published without merit, and it seems that would be all the worse in publishing of models and code.  "
"I think that a stronger recognition of computational research outputs (script, software, testcases, dataset, etc etc) in terms of career visibility/indexing/scoring is needed. Otherwise researchers have no time and resources to both publish ""standard"" scientific contributions and also reproducible comp. research. This has to pay back in terms of career development and cv, so it needs full recognitions by Insitutions, Journals and academic performance metrics."
Best practice guidelines specific to the use of particular kinds of codes (e.g. computational fluid dynamics). But such guidelines need to be sensitive to the kinds of questions asked of models in the geosciences and not the kinds of highly idealised problems that are characteristic of some applications (w.g. where boundary conditions are perfectly known).
Well documented applications that also include e.g how to set it up on eg a windows computer. Only this way this becomes accessible to users rather than staying in academic institutions. 
   use open science
"I think in general we shall be more confident about the codes we use and develop to share them also more openly. More education and workshops about coding would help.
"
""
I don't know!
support more software engineering support to researchers
""
"Speaking for myself, I have never had formal training in code development, so I assume my code is usually inefficient. But it works for the task at hand. If I knew that there would be a large group of people wanting to use or reproduce my model, it might make sense to work with a programmer/developer to clean, optimize, and document the code, and deliver it with sample datasets. But my perception is that there are few people who would bother to try to reproduce my work. I do make an effort (when public data is part of the model inputs) to provide the code in a reproducible format like an R Notebook. But the majority of my code is theoretical and has no external inputs."
"Funding opportunities are great, but are yet another thing to apply for and maybe or maybe not get.
My biggest struggle is trying to reproduce a result by following the model description in a paper that does not also include the code. Often I am not able to because the description is incomplete. But it is a difficult thing to catch in review because it is so time-intensive. 

If the code is released alongside the paper and is well-documented, it can be tested much more efficiently, and discrepancies between the code and the description can be caught. But even that is above the level of time investment of a typical journal review. Requiring code be made avaiable to reviewers (even if not released with the publication) would help."
""
We should develop curriculum for reproducibility that grad students should be encouraged to take. We should have seminars at conferences devoted to reproducibility. We should encourage the creation of reproducibility working groups at institutions that can serve as a resource to their colleagues and offer occasional workshops
Share datasets used
Need to include more programming / coding classes at the undergraduate level. Tailored to earth and environmental sciences. Don't learn coding when you are a postdoc!
"Take the time to think about how we can keep our work reproducible.
Best pracctices in universitites/groups.
More courses on e.g. GIT."
Share everything
We should all have more time at hand and less pressure to keep publishing.
-
Promote it as tools and make in applicable for non-IT-experts
Provide open access codes and documentations. Publish codes and tutorials
"Train students early on proper computer science techniques, methods, and documentation. The biggest problem is that few of us really know what we're doing--we write code that is confusing and poorly documented. I also think that if there were more pressure to publish open-source, it would encourage researchers to provide better documentation."
"We need a ""standards and ecosystem"" approach. Scientists need to be cognizant of how their software will work with software and techniques from other scientists. This means established formats for common data, and tools for completing common data-processing steps."
"I think we should put more emphasis on open source code sharing during publication. Now almost every journal asks to give a statement on data availability, they should do the same with code availability. Then it would be much clearer to the readers where to find the code or that the authors give the posibility to ask for the code. "
"Sharing workflows for modeling, with *all* assumptions. Often, scientific modeling workflows have un-documented steps (e.g. manual data cleanup) that prevent reproducibility - and these are places where errors easily creep in.

We should also normalize code review *well* before the peer review stage. These are standard industry practices with lots of evidence to show that they reduce bugs and improve the development process. "
"Design curriculum for a complete course(s) to be taken as upper-level, but definitely by grad school. Have sessions at AGU centered around doing science for scientists and science - reproducibility in programming is just one arm; it also needs to be discussed for collecting data, field methods - students are not taught even how to think in terms of designing and communicating workflows, so we have a long way to go. "
"Several of my colleagues have sepnt their career to develop certain codes for their own research. They consider these as their livelihood, and would never surrender them to others. 

I find that the codes that I produce for my research are easy enough for anyone interested to reproduce based on the math that is presented in my papers. Besides, those who could not reproduce the code, would certainly not be able to run the ocde either, even if i gave it to them."
I don't know. I'm not sure that reproducability is a problem as the code we write isn't close to capturing the complexity of the phenomena that we're observing.
"All computation-related publications should require codes to be available (including all relevant inputs) at the time of publication, with the option to embargo the ability to download those codes for a short duration to protect from other research getting scooped. Introducing software methods in papers without including the actual code leaves the work of reproducing those codes up to less skilled and uninformed graduate students, who are much more likely to apply the methods erroneously. Nobody doing active computational research in our fields should be publishing computational research for which the codes are not available, and the culture needs to reflect that."
Raise awareness about the importance of reproducibility in science.
"I have not any idea to propose, but it is a serious problem"
"More support and opportunities to work with someone good at coding to improve my own coding and to edit it to be more shareable. Also requiring all undergraduates and graduate students to take a class in coding and documentation. I'm such a hack because I really was just picking stuff up ad hoc. Without knowing how to code, you are a little illiterate in this day and age. So, requiring coding like we require writing courses makes a lot of sense."
"Have more software engineers (scientific software developers) active in research teams (i.e., part of the section staff, from other departments or eScience centers, external collaborators, etc.). Software development is a very useful skill to develop for a researcher, but more and more an additional requirement that adds a lot of burden on researchers alongside other skiils like admin, management, teaching, writing, etc. Software becomes more and more complex (design patterns, handling big datasets, running code on cloud/HPC, CPU/GPU, managing dependencies, legal aspects of publish code, rapidly evolving software ecosystems, interactiing with open-source communities, etc.) and it is too much to rest on the back of researchers / scientists only, in my opinion.

Involve software developers early in the research workflow (i.e., as soon as writing a new projects, not for cleaning the code just before submitting or publishing a paper) so that they can work jointly with researchers on projets, focusing on software design, offering personalized support and giving advices on the technologies that are best suited to a given problem. This is, in my opinion and from my experience, very effective (much less work burden, mutual motivation and suggestions) and highly rewarding for both software developers and researchers.

Offer more career development opportunities for scientific software developers in the acamedia.

Better inform researchers about open-source code publishing (lots of researchers are still afraid of publicly sharing their own code and/or that someone else will steal their code)."
N/A 
"increase best practices workshops, provide a programer for each research group. Sometimes, the time is not efficiently used because researchers with poor programmer skills use more time than an actual programmer. "
Make it a part of reviewing process for publications
X
"Hmm... I believe that code is often rewritten by many researchers. As a result, many resources are wasted on developing new code. While the creation of new code is important, especially for early-career scientists, I think great resources should be put towards developing generalizable and robust code that can be applied by many "
Make reproducing other work a minimum standard for tenure review or graduate student review. The pressure to produce novel results to the exclusion of replication will never permit strong motivation. Funding agencies also have to act in concert.
Funding for this opportunity. 
"Value the time it takes to document code and computational workflows - and these efforts must count towards promotion in the same way as journal articles. 

Recognize that papers, code, and workflows might all need individual review and that each of these reviews take different skillsets. An example might be paired publications between a traditional journal and a journal focused on the software such as the Journal of Open Source Software). 

Careful consideration of what artifacts must be archived for reproduciblility (e.g., input files, pre-processing steps, etc) and appropriate locations for such artifacts. The EarthCube ModelData RCN has made some nice progress here. https://modeldatarcn.github.io/rubrics-worksheets/rubrics.html

A potentially controversial comment, which comes from the perspective of a research scientist who often develops software for new applications rather than using software: I think all computational code, computational environment, and worksflows, should be documented such that the author can reproduce their work, and such that an independent individual willing to put in the effort can reproduce the work. When the investigator is doing science, creating software, and undertaking the computation, it is too much to ask that all work is documented to a high level such that it is EASY for an independent individual to reproduce the work. "
"First the data should be published along with the publication, then comes the code. Code without the source data does not really help."
"Advocate for more transparency and documentation of code, making it readily and easily available as a extra resource in peer-reviewed publications."
"Motivation in the form of credit or publications.  The pressure for publication is insane these days.  If you don't have a permanent position, putting effort into maintaining software for external users, many of whom lack relevant skills, means losing time to publish papers and keep your job."
-
Be part of the publication process. Availability of easy to use tools to facilitate reproducibility (e.g. cloud environment).
Write better code and better documentation
"Mandatory data and input files submission during submission to a journal.
More options for publishing software: a DOI is not enough for 1+ more years of research. ""Classic"" journals are generally not interested in publishing software papers."
"- Publish papers as open access.
- Always publish code and data with papers under an open access license.
- Open source climate models."
"Wrapping up data, code and results in a reproducible manner requires lots of effort. Lack of funding/time for that might be the main bottleneck for me"
"Force authors to provide the data and their code on publication.

And have th code reviewed by dedicated code peer-review."
"Force journals to allow the publication of reproducibility studies, which, in sedimentology/basin analysis, is basically impossible.

And convince grant people that it could be beneficial to pay for reproducibility study, because it is impossible to get funding for ""redoing something that was already done before""."
"Code more readable and simple and use online computation environments to share their own simulation environment, e.g. MyBinder"
Explore a badging scheme for published papers that include reproducible code or data.
"Share all model code with sufficient documentation. This documentation should also include an overview over the structure of the model (which files are used for what) because otherwise it is very hard to understand. It often takes me a long time to find the model code online and if I find it, it is often very poorly documented, which means you would have to reach out to the authors for an explanation (e.g. the very often used PCR-GLOBWB https://github.com/UU-Hydro/PCR-GLOBWB_model). I think there should be more funding and credit allocated to documenting code.

Also, I think improving the testing of models should be a focus point. I have the feeling many people do not know how to write tests that will automatically be run for every model update. This makes code very prone to errors that may remain undetected for a long time. I think there should be more workshops given on this topic and software testing should be compulsory for models that are widely used."
"- enforce clear and transparent guidelines by employer / university

- asking for code / data avilability declarations (as some journals do by now) is good but code review should also be enforced (in my case it was never done even when asked by the journal, e.g. GMD)

- code review would make review process even more time demanding -> publishers could pay reviewers (I often wonder what APCs are actually good for)

- higher emphasis of code development and reproducibility at university (already at BSc level)"
"- offer teaching on useful tools for reproducibility of computational research in study programs (e.g. version control system, automated tests, ?)"
"I think a code review as part of peer-review process would be very helpful but not possible for, e.g., large hydrological models. "
na
"make it compulsory, provide time and money to make computational reserach reprodubile, reward the work"
"I have found that the codes from the research domains of computer science, electrical engineering, mathematics are easily available on github and hence reproducible. They have simple datasets, unlike earth science papers in which there are lots of variations in the datasets depending upon the geography etc. The earth science papers can be divided into two parts, one includes pure alorithm development and the other as an applications of the developed algorithm."
Scientific Journals should only accept manuscripts with well-documented workflows/code. Well-documented and well-structured code/workflow requires a lot of time and is barely acknowledged. Most of the times scientiests are evaluated by the number of publications. Code Quality is not considered for evaluation of the scientific work.
"Additional funding/time for thotrough documentation of code.
Mandatory publication of codes used for pubilcations.
"
practice reproducibility as part of scientific work 
.
